# Created 2024-08-16 Fri 20:15
#+title: 
#+author: Harry Askham
* Owt

Serve up owt yer fancy on t'fly.
** What's This?

A Flask app exposing an endpoint whose request handling behaviour is configured by the request itself.

For example:

#+begin_src python
# adaptor.py
from big.research.lab import fancy_ai

def run(_, prompt):
    return fancy_ai.do_something(prompt)
#+end_src

Which can be called from any language with a tiny client:

#+begin_src javascript
// client.js
const url = owtify('adaptor.py', {'prompt': 'output SOTA results'});
const response = await fetch(url);
// ...
#+end_src
*** Security
*This works by evaluating arbitrary user-supplied strings as code. This is grossly insecure by its very nature; don't expose it to the internet!*

There's support for basic auth via ~--auth="username:sha256(password)"~ but still, exercise caution. It would not be difficult to accidentally make an ~owt~ API call that irreversibly destroyed your own machine.
*** Why?

The primary motivation is rapid prototyping against new machine learning models from languages other than Python. So often the newest hotness drops with a checkpoint and a Python client, and using this outside of the specific ecosystem in which it is designed to run means one of:

1. FFI-wrapping the library to get native-looking calls in your language (or use an embedded Python if your language has it)
   - Painful and bespoke-to-each-new-library work to figure out how to make all necessary requirements, virtualenv setup, CUDA flags, etc available.
   - Or, now your software only works inside a virtualenv.
2. Spawn ~python some_project.py --flag=...~" as a child process
   - Handle communication over stdin/out/err can be a pain.
3. Docker-ify it
   - Long; CUDA can be annoying; still need to expose the logic by one of the other methods.
4. Building a lightweight API backend exposing the Python logic and writing client code in your language of choice
   - Annoying boilerplate; the time between "I can run ~python generate.py --animal=cat > pic.png~ on my machine" and "I have some ~generate :: Animal -> IO Image~" can be >1h.
   - Either one new service for every new library you want to test out, or now you need to maintain a growing monolith of unrelated endpoints

~owt~ aims to make #4 less painful, at the cost of security and sanity, by providing a single service capable of serving arbitrary new logic without requiring changes to ~owt~ itself.
*** How?
The cost of supporting some new system is pushed to the client, who must send adaptor code (a request handler, essentially) along with the request. This creates a virtual endpoint on the fly, giving complete control over the serving logic to the API user.

Writing the adaptor code is a one-time cost for each new virtual endpoint, made cheaper by having access to ~owt.summat~, a collection of composable building blocks.
*** Examples

The following examples (in ~example/~) could all be run one-by-one without any need to restart or rebuild ~owt~. The first one is shown a few different ways to give a flavour of usage. Subsequent examples just show ~curl~ in tandem with an adaptor ~.py~ file, but it's easy to see how one could extend from here to call to ~owt~ from any other language.
**** Echo Server
By default, the function named ~run(request, **kwargs)~ in the user-supplied code will be used to handle the request.
Code and (optionally) arguments are supplied as ~code_b64~ and ~kwargs_b64~. ~kwargs_b64~ is ~eval~'d to get the resulting dictionary, so can itself contain richer logic to build up arguments.
***** As a self-contained shell script
#+begin_src bash
URL="$1"

read -r -d '' CODE << EOF
def run(_, name: str):
  return f'Hello, {name}!'
EOF

curl --json $(jo \
  code_b64=$(echo "$CODE" | base64 -w 0) \
  kwargs_b64=$(echo "{'name': 'owt'}" | base64 -w 0) \
) $URL/hello
#+end_src

#+begin_src bash :exports both :results html
./example/echo/echo_script.sh http://localhost:9876
#+end_src
***** As a Python file + script
#+begin_src python
def run(request, name=None):
    # Loads from either the free-text path or the request body
    if not name:
        name = request.path.split("/")[-1]
    return f"Hello, {name}!"
#+end_src

Passing data via POST JSON ~kwargs~:

#+begin_src bash
URL="$1"
CODE_B64=$(cat example/echo/echo.py | base64 -w 0)
KWARGS_B64=$(echo "{'name': 'owt'}" | base64 -w 0)
curl --json $(jo code_b64=$CODE_B64 kwargs_b64=$KWARGS_B64) $URL/hello
#+end_src

#+begin_src bash :exports both :results html
./example/echo/echo_kwargs.sh http://localhost:9876
#+end_src

Passing data via GET in the path:

#+begin_src bash
URL="$1"
CODE_B64=$(cat example/echo/echo.py | base64 -w 0)
KWARGS_B64=$(echo "{'name': 'owt'}" | base64 -w 0)
curl -G --data-urlencode code_b64=$CODE_B64 --data-urlencode kwargs_b64=$KWARGS_B64 $URL
#+end_src

#+begin_src bash :exports both :results html
./example/echo/echo_request.sh http://localhost:9876
#+end_src
**** Text to Speech API
A more complex example demonstrating wrapping Suno's OSS TTS model (https://github.com/suno-ai/bark)[Bark].

The client provides an adaptor that responds with a stream of bytes, allowing the generated audio to be streamed in chunks, sentence-by-sentence.

Responses are cached for the lifetime of the ~owt~ server for each combination of ~(text, speaker)~.

The ~preload_models()~ call makes the first call take a while as VRAM is populated, but the weights remain in memory so subsequent calls are cheaper.

To avoid this breaking other ~owt~ uses, one can spin up multiple instances of ~owt~, each handling a different kind of task and with different resource profiles.
***** Python Adaptor
The endpoint logic, to be base64-encoded as part of the request.
#+begin_src python
def run(request, text: str, speaker: str = "v2/en_speaker_6"):
    import os
    import logging
    import io
    import nltk
    from scipy.io.wavfile import write as write_wav

    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    os.environ["SUNO_USE_SMALL_MODELS"] = "0"
    os.environ["SUNO_OFFLOAD_CPU"] = "0"

    from bark.generation import generate_text_semantic, preload_models
    from bark import generate_audio, SAMPLE_RATE

    preload_models()

    def generate():
        sentences = nltk.sent_tokenize(text.replace("\n", " ").strip())
        for i, sentence in enumerate(sentences):
            logging.info(
                "Generating sentence %d/%d: %s", i + 1, len(sentences), sentence
            )
            wav_array = generate_audio(sentence, history_prompt=speaker)
            buf = io.BytesIO()
            write_wav(buf, SAMPLE_RATE, wav_array)
            yield buf.read()

    return generate(), {"Content-Type": "audio/mpeg"}
#+end_src
***** Save audio via cURL
Bundle the endpoint logic with a prompt and download the resulting audio.
#+begin_src bash
# Usage:
# ./example/bark/bark.sh \
#   http://localhost:9876/file.wav \
#   "Hello world! This is a test." \
#   /tmp/output_file.wav

URL="$1"
TEXT="$2"
OUTFILE="$3"

CODE="$(< example/bark/bark.py)"
KWARGS="{'text': '$TEXT'}"
JSON=$(jo \
  code_b64=$(echo "$CODE" | base64 -w 0) \
  kwargs_b64=$(echo "$KWARGS" | base64 -w 0) \
  use_cache="true" \
  cache_kwargs="true" \
)
CMD="curl --json $JSON $URL -o $OUTFILE"

echo "Running $CMD"
$CMD
echo "Wrote $OUTFILE"
#+end_src
***** Stream audio via JS
Use an endpoint from a webapp - see ~example/bark/bark.html~ for usage.
#+begin_src javascript
function makeRequest(code, text) {
  return {
    'code_b64': btoa(code),
    'kwargs_b64': btoa('{"text": "' + text + '"}')
  };
}

async function getAudio(url, code, text, onChunk, onDone) {
  let response = await fetch(url, {
      method: 'POST',
      headers: {
          'Content-Type': 'application/json',
      },
      body: JSON.stringify(makeRequest(code, text))
  });
  if (!response.ok) {
    throw new Error(await response.text());
  }
  const reader = response.body.getReader();
  reader.read().then(({ done, chunk }) => {
    if (chunk) {
      onChunk(chunk);
    }
    if (done) {
      onDone();
    }
  });
}
#+end_src
***** Ad-hoc Web Server
In fact we can go one step further now and bootstrap our own webserver within ~owt~ to serve our prototype app.

We ca create an adhoc endpoint that serves us the rendered ~bark.html~ Jinja2 template.

The ~owt~ arguments can be passed as GET query parameters as well as POST JSON data, so we can actually write a handler that embeds the entire HTML into the query with this Python-in-Python-in-Bash curiosity.

#+begin_src bash
URL="$1"
CODE=$(python <<EOF
with open('example/bark/bark.html', 'r') as html_f:
  html = html_f.read()
  with open('example/bark/bark.py', 'r') as code_f:
    code = code_f.read()
    with open('example/bark/bark.js', 'r') as js_f:
      template = (html.replace('{% include "bark.py" %}', code)
                  .replace('<script src="/bark/bark.js"></script>',
                           '<script>\n'+js_f.read()+'\n</script>'))
      print('''
def run(_):
  from flask import render_template
  try:
      render_template(\'\'\''''+template+'''\'\'\')
  except Exception as e:
      return str(e)''')
EOF
)
CODE_B64=$(base64 -w 0 <<< "$CODE")
echo "curl -G --data-urlencode \"code_b64=$CODE_B64\" $URL"
#+end_src

#+begin_src bash :exports both :results html
bash -c "$(./example/bark/bark_construct_curl.sh http://localhost:9876) -s -o /dev/null -w '%{url}'"
#+end_src

Whew. We can open that ~owt~ URL in a browser and play with the web app, which itself makes calls to ~owt~ injecting the TTS logic.
***** Going Meta

That gets painful though - for iterative development, you want to save your code and hit refresh. This won't do anything here, since all code is snapshotted into the URL itself. However...

#+begin_src bash
URL="$1"
read -r -d '' CODE << 'EOF'
def run(_, base_url):
  import os
  html = os.popen(f'bash -c "$(./example/bark/bark_construct_curl.sh {base_url})"').read()
  return html
EOF

KWARGS="{\"base_url\": \"$URL\"}"
CODE_B64=$(base64 -w 0 <<< "$CODE")
KWARGS_B64=$(base64 -w 0 <<< "$KWARGS")
echo "curl -G --data-urlencode code_b64=$CODE_B64 --data-urlencode kwargs_b64=$KWARGS_B64 $URL"
#+end_src

#+begin_src bash :exports both :results html
./example/bark/bark_meta_curl.sh http://localhost:9876
#+end_src

Sweet - this will resolve to the meta-evaluator that always renders a fresh copy of the app each time.

#+begin_src bash :exports both :results html
bash -c "$(./example/bark/bark_meta_curl.sh http://localhost:9876) -s -o /dev/null -w '%{url}'"
#+end_src
**** Going Meta-Circular

Wait, no...

#+begin_src bash
function owtInOwt() {
  URL="$1"
  PORT="$2"
  PAYLOAD_CODE_B64="$3"
  PAYLOAD_KWARGS_B64="$4"
  read -r -d '' CODE << EOF
def run(request, payload_code_b64, payload_kwargs_b64):
  _globals = {'__name__': __name__+'_new',
              'new_port': args.port + 1}
  _locals = {}
  print(f'Going one level down to port {_globals['new_port']}...')

  exec('''
print('One level deeper, importing owt')
from owt import *
from multiprocessing import Process
args.port = new_port
server_thread = Process(target=main)
''', _globals, _locals)

  def kill():
    import time
    time.sleep(10)
    print(f'Killing server on {args.port}')
    _locals['server_thread'].terminate()
    print('Killed server on %d' % args.port)

  from multiprocessing import Process
  import requests
  import urllib

  _locals['server_thread'].start()
  bootstrapped_url = f"$URL:{_globals['new_port']}/{request.path}?code_b64={urllib.parse.quote_plus(payload_code_b64)}&kwargs_b64={urllib.parse.quote_plus(payload_kwargs_b64)}"
  print(bootstrapped_url)
  resp = requests.get(bootstrapped_url).content
  Process(target=kill).start()
  return resp
EOF

  CODE_B64=$(base64 -w 0 <<< "$CODE")
  KWARGS_B64=$(base64 -w 0 <<< "{\"payload_code_b64\":\"$PAYLOAD_CODE_B64\", \"payload_kwargs_b64\": \"$PAYLOAD_KWARGS_B64\"}")
  CMD="curl -G --data-urlencode code_b64=$CODE_B64 --data-urlencode kwargs_b64=$KWARGS_B64 $URL:$PORT"
  echo $CMD
}
#+end_src

Oh no, no...

#+begin_src bash :exports both :results html
source "example/meta/bootstrap.sh"

# Load up the nice simple echo example from earlier
CODE_B64=$(cat example/echo/echo.py | base64 -w 0)
KWARGS_B64=$(echo "{'name': 'owt-inside-owt'}" | base64 -w 0)

# Send a request that installs a full copy of owt and calls it with the payload code+kwargs
CMD=$(owtInOwt http://localhost 9876 "$CODE_B64" "$KWARGS_B64")
echo "Bootstrapping $CMD"
echo "Result:"
bash -c "$CMD"
#+end_src

Oh no... but that would mean...

#+begin_src python
def run(request, payload_code_b64, payload_kwargs_b64):
    import os

    return os.popen(
        f'source ./example/meta/bootstrap.sh; $(owtInOwt http://localhost {args.port} "{payload_code_b64}" "{payload_kwargs_b64}")'
    ).read()
#+end_src

#+begin_src bash :exports both :results html :noeval
METACODE_B64=$(cat example/meta/bootstrap.py | base64 -w 0)
function wrapOwt() {
  CODE_B64="$1"
  KWARGS_B64="$2"
  METAKWARGS_B64=$(base64 -w 0 <<< "{\"payload_code_b64\":\"$CODE_B64\", \"payload_kwargs_b64\": \"$KWARGS_B64\"}")
  echo "$METAKWARGS_B64"
}

N_LAYERS="10"
for layer in $(seq 1 $N_LAYERS); do
  CODE_B64=$(cat example/echo/echo.py | base64 -w 0)
  NAME="owt"
  for i in $(seq 1 $layer); do
      NAME="$NAME-inside-owt"
  done
  KWARGS_B64=$(echo "{\"name\": \"$NAME\"}" | base64 -w 0)
  METAKWARGS_B64=$(wrapOwt "$CODE_B64" "$KWARGS_B64")
  for i in seq 2 $layer; do
      METAKWARGS_B64=$(wrapOwt "$METACODE_B64" "$METAKWARGS_B64")
  done
  echo "layer: $NAME"
  CMD="curl -G --data-urlencode code_b64=$METACODE_B64 --data-urlencode kwargs_b64=$METAKWARGS_B64 http://localhost:9876"
  echo "Result: " $(bash -c "$CMD")
done
#+end_src

Hoo boy. How is Python a real language.
